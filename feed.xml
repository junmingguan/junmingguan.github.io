<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://junmingguan.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://junmingguan.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-30T07:08:12+00:00</updated><id>https://junmingguan.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Empirical Bayes Confidence Intervals</title><link href="https://junmingguan.github.io/blog/2025/ebci/" rel="alternate" type="text/html" title="Empirical Bayes Confidence Intervals"/><published>2025-04-28T21:30:00+00:00</published><updated>2025-04-28T21:30:00+00:00</updated><id>https://junmingguan.github.io/blog/2025/ebci</id><content type="html" xml:base="https://junmingguan.github.io/blog/2025/ebci/"><![CDATA[<p><em><strong>In progess</strong></em>. This note summarizes the methods proposed by Ignadiatis and Wager 2022.</p> <h5 id="setup">Setup</h5> <p>Suppose we have following model</p> \[\begin{equation} Z_i = \mu_i + \epsilon_i, \hspace{1cm} \mu_i \stackrel{iid}{\sim} G\in \mathcal{G}, \hspace{1cm} \epsilon_i \stackrel{iid}{\sim} \mathcal{N}(0,1), \end{equation}\] <p>for some prior class $\mathcal{G}$. We are interested in estimating $\theta_G(z)=\mathbb{E}_G[h(\mu)\mid Z=z]$ for some function \(h\) and forming confidence intervals for the estimates. Ignadiatis and Wager propose two approaches for this task: \(F\)-localization and AMARI.</p> <h5 id="f-localization">$F$-localization</h5> <p><strong>Idea</strong>: find a level-$\alpha$ set of distribution functions $\mathcal{F}_n(\alpha)$ for the true marginal distribution function $F_G$, i.e.,</p> \[\lim \inf_{n\to \infty} \mathbb{P}_G\Big[ F_G \in \mathcal{F}_n(\alpha)\Big] \geq 1-\alpha.\] <p>For $z$ of interest, identify all $\hat{G}$ such that $F_{\widehat{G}}$ lies inside \(\mathcal{F}_n(\alpha)\). Now that we have a set of $\theta_{\widehat{G}}(z)$â€™s, we can simply take and maximum and minimum to form a confidence band.</p> <h5 id="amari">AMARI</h5> <p><strong>Prerequisite: bias-aware confidence intervals</strong></p> <p>Suppose we have some estimate $\hat{m}$ of some functional $m \in \mathcal{M}$ with standard error ${se}(\hat{m}(x_0))$. Denote the worse case bias by $B = \text{sup}_{m \in \mathcal{M}}|\hat{m}(x_0) - m(x_0)|$. Then a naive confidence interval for $m(x_0)$ is</p> \[\hat{m}(x_0) \pm (B + t_{1-\alpha/2} se(\hat{m}(x_0))).\] <p>To make the length of the CI as short as possible, a better one is given by</p> \[\hat{m}(x_0) \pm t_\alpha (se(\hat{m}(x_0)), B),\] <p>where</p> \[t_\alpha (se(\hat{m}(x_0)), B) = \inf \big\{ t &gt; 0: \forall |b| \leq B: \mathbb{P}[|Z_b| \leq t] \geq \alpha\big\},\] <p>and $Z_b \sim \mathcal{N}(b, se(\hat{m}(x_0))^2)$.</p> <p><strong>Non-identifiability in the Bernoulli Model</strong></p> <p>Consider the model $Z_i \mid \mu_i \sim \text{Bernoulli}(\mu_i)$, and $\mu_i\sim G\in\mathcal{P}([0,1])$. Then we have $p(Z_i \mid \mu_i) = \mu_i^{z_i}(1-\mu_i)^{1-z_i}$, $ f_G(1) = \int \mu dG(\mu)$, and $f_G(0) = 1 - f_G(1)$. So the marginal distribution \(f_G\) of \(Z\) is determined by \(f_G(1)\).</p> <p>The second moment \(L(G)=\int \mu^2 dG(\mu)\) is not point-identified. Different priors \(G\) with the same \(f_G(1)\), \(L(G)\) could have different \(L(G)\). The maximum is attained at \(G\) such that \(\PP_G[\{1\}]=f_G(1)\) and \(\PP_G[\{0\}] = f_G(0)\). In this case, \(L(G) = \text{Var}_G(\mu)+(f_G(1))^2 = f_G(1)\). The minimum is attained at \(\PP_G[\{f_G(1)\}]=1\), with \(L(G) = f_G(1)^2\).</p>]]></content><author><name></name></author><category term="notes"/><category term="empirical-bayes"/><category term="statistics"/><summary type="html"><![CDATA[Constructing confidence intervals for EB estimates]]></summary></entry><entry><title type="html">Poisson f-modeling</title><link href="https://junmingguan.github.io/blog/2025/poisson-f-modeling/" rel="alternate" type="text/html" title="Poisson f-modeling"/><published>2025-04-12T21:30:00+00:00</published><updated>2025-04-12T21:30:00+00:00</updated><id>https://junmingguan.github.io/blog/2025/poisson-f-modeling</id><content type="html" xml:base="https://junmingguan.github.io/blog/2025/poisson-f-modeling/"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
    Macros: {
      RR: "{\\mathbb{R}}",
      EE, "{\\mathbb{E}}",
      vec: ["{\\mathbf{#1}}", 1],
      abs: ["\\left|#1\\right|", 1]
    }
  }
});
</script> <p>Suppose \( Z\sim \text{Poi}(\theta)\) and \(\theta\sim G\in \mathcal{P}(\mathbb{R}_+)\). The posterior mean of \(\theta \mid Z=z\) is given by: \(\mathbb{E}[\theta \mid Z=z] = \frac{(z+1)f_G(z+1)}{f_G(z)},\) where \(f_G(z) = \int e^{-\theta} \theta^{z}/z!dG(\theta).\)</p> <p>We want to show that \(\mathbb{E}[\theta \mid Z=z]\) is monotonic in \(Z=z\). To do so, we want to show \(\frac{\mathbb{E}[\theta \mid Z=z]}{\mathbb{E}[\theta \mid Z=z-1]} \geq 1,\) which amounts to showing</p> \[\frac{\int e^{-\theta} \theta^{z+1}dG(\theta)}{\int e^{-\theta}\theta^{z}dG(\theta)} \geq \frac{\int e^{-\theta} \theta^{z}dG(\theta)}{\int e^{-\theta} \theta^{z-1}dG(\theta)}.\] <p><strong>Idea</strong>: Consider the measure \(dH_{z}(\theta) = e^{-\theta} \theta^{z}dG(\theta)\). It boils down to showing \(h(z) = \int dH_z\) is log-convex, so that \(\text{log}(h(z+1))+\text{log}(h(z-1)) \geq 2\text{log}(h(z))\).</p> <p>Note that \(\frac{d }{dz}h(z) = \int e^{-\theta} \theta^{z}\text{ln}\theta dG(\theta) = \mathbb{E}_{H_{z}} [\text{ln}\theta].\)</p> <p>This is because the function \(a(\theta) = e^{-\theta} \theta^{z} \text{ln} \theta\) is bounded (think about the Gamma function) hence integrable with respect to \(G\), a probability measure, so one can differentiate the expectation (see Klenke). Similarly, \(e^{-\theta} \theta^{z} (\text{ln} \theta)^2\) is bounded, and thus \(\frac{d^{2} }{dz^{2}}h(z) = \int e^{-\theta} \theta^{z}(\text{ln}\theta)^{2} dG(\theta) = \mathbb{E}_{H_{z}} [(\text{ln}\theta)^{2}].\)</p> <p>By Cauchy-Schwarz, we have \((\mathbb{E}_{H_{z}} [\text{ln} \theta])^{2} = h^{(1)}(z)^{2} \leq \mathbb{E}_{H_{z}}[1]\mathbb{E}_{H_{z}}[(\text{ln} \theta)^{2}] = h(z)h^{(2)}(z).\)</p> <p>So \(h(z)\) is log-convex, due to the fact that</p> \[\big( \text{log}(h(z)) \big)^{''} = \frac{h(z)h^{(2)}(z)-h^{(1)}(z)^2}{h^{(1)}(z)^{2}} \geq 0.\]]]></content><author><name></name></author><category term="notes"/><category term="empirical-bayes"/><category term="math"/><summary type="html"><![CDATA[Monotonicity of Poisson posterior mean]]></summary></entry></feed>